{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a03aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import torch\n",
    "from numpy._core.multiarray import scalar\n",
    "from models.change_classifier import ChangeClassifier\n",
    "\n",
    "# Add numpy scalar to safe globals BEFORE loading\n",
    "torch.serialization.add_safe_globals([scalar])\n",
    "\n",
    "model = ChangeClassifier(weights=None)\n",
    "\n",
    "# Now load with weights_only=True (safe mode)\n",
    "ckpt = torch.load(\"../pretrained_models/checkpoint_086.pth\", weights_only=False)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), \"../pretrained_models/model_entire.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6ba0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1 dimensions: [40, 48, 80]\n",
      "Mixing blocks for B1:\n",
      "Layer 1: MixingMaskAttentionBlock(40, 20, [20, 10, 5], [10, 5, 1])\n",
      "Layer 2: MixingMaskAttentionBlock(48, 24, [24, 12, 6], [12, 6, 1])\n",
      "Layer 3: MixingMaskAttentionBlock(80, 40, [40, 20, 10], [20, 10, 1])\n",
      "\n",
      "Up dims for B1: [(2, 40, 48), (2, 48, 40), (2, 40, 32)]\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.efficientnet import EfficientNet\n",
    "\n",
    "def get_exact_efficientnet_channels(model_name):\n",
    "    # Create model to extract exact channel dimensions\n",
    "    model = torchvision.models.__dict__[model_name](weights=None)\n",
    "    \n",
    "    channels = []\n",
    "    for name, module in model.features.named_children():\n",
    "        # For MBConv blocks, get the output channels\n",
    "        if hasattr(module, 'block') and hasattr(module.block, 'out_channels'):\n",
    "            channels.append(module.block.out_channels)\n",
    "        # For the first conv layer\n",
    "        elif name == '0' and hasattr(module, 'out_channels'):\n",
    "            channels.append(module.out_channels)\n",
    "    \n",
    "    return channels\n",
    "\n",
    "# Get exact channel dimensions for both models\n",
    "b4_channels = get_exact_efficientnet_channels('efficientnet_b4')\n",
    "b1_channels = get_exact_efficientnet_channels('efficientnet_b1')\n",
    "\n",
    "print(f\"B4 channels: {b4_channels}\")\n",
    "print(f\"B1 channels: {b1_channels}\")\n",
    "\n",
    "# Calculate the exact scaling factors\n",
    "scaling_factors = [b1 / b4 for b1, b4 in zip(b1_channels, b4_channels)]\n",
    "print(f\"Scaling factors: {[f'{x:.3f}' for x in scaling_factors]}\")\n",
    "\n",
    "# Get the specific layers we need (assuming layers 1, 2, 3 as in your original code)\n",
    "b4_target_dims = [48, 64, 112]  # From your original MixingMaskAttentionBlock\n",
    "b1_target_dims = [b1_channels[i] for i in [1, 2, 3]]  # Corresponding layers in B1\n",
    "\n",
    "print(f\"\\nB4 target dimensions: {b4_target_dims}\")\n",
    "print(f\"B1 target dimensions: {b1_target_dims}\")\n",
    "\n",
    "# Generate the MixingMaskAttentionBlock parameters\n",
    "print(\"\\nMixing blocks for B1:\")\n",
    "for i, dim in enumerate(b1_target_dims):\n",
    "    in_channels = dim\n",
    "    out_channels = dim // 2\n",
    "    print(f\"Layer {i+1}: MixingMaskAttentionBlock({in_channels}, {out_channels}, \"\n",
    "          f\"[{out_channels}, {out_channels//2}, {out_channels//4}], \"\n",
    "          f\"[{out_channels//2}, {out_channels//4}, 1])\")\n",
    "\n",
    "# Generate up_dims\n",
    "print(f\"\\nUp dims for B1: [(2, {b1_target_dims[2]//2}, {b1_target_dims[1]}), (2, {b1_target_dims[1]}, {b1_target_dims[0]}), (2, {b1_target_dims[0]}, 32)]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b51e1802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet_b4: 285,803 parameters - ✅ Loaded successfully\n",
      "efficientnet_b3: ❌ Failed to load - Invalid Weight class provided; expected EfficientNet_B3_Weights but received EfficientNet_B4_Weights.\n",
      "efficientnet_b2: ❌ Failed to load - Invalid Weight class provided; expected EfficientNet_B2_Weights but received EfficientNet_B4_Weights.\n",
      "efficientnet_b1: ❌ Failed to load - Invalid Weight class provided; expected EfficientNet_B1_Weights but received EfficientNet_B4_Weights.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "# Quick test script\n",
    "from models.change_classifier import ChangeClassifier\n",
    "\n",
    "# Test all backbones\n",
    "for backbone in [\"efficientnet_b4\", \"efficientnet_b3\", \"efficientnet_b2\", \"efficientnet_b1\"]:\n",
    "    try:\n",
    "        model = ChangeClassifier(bkbn_name=backbone)\n",
    "        param_count = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"{backbone}: {param_count:,} parameters - ✅ Loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"{backbone}: ❌ Failed to load - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd267861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys: dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'loss'])\n",
      "Model state dict keys sample: ['_retina.dog.weight', '_backbone.0.0.weight', '_backbone.0.1.weight', '_backbone.0.1.bias', '_backbone.0.1.running_mean']\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Try to identify what exactly is causing the issue\n",
    "from models.change_classifier import ChangeClassifier\n",
    "\n",
    "# Create a simple test to see what's in the checkpoint\n",
    "try:\n",
    "    # Try to peek at the checkpoint structure without loading numpy scalars\n",
    "    checkpoint = torch.load(\"../pretrained_models/checkpoint_086.pth\", \n",
    "                           weights_only=False)\n",
    "    \n",
    "    print(\"Checkpoint keys:\", checkpoint.keys())\n",
    "    print(\"Model state dict keys sample:\", list(checkpoint['model_state_dict'].keys())[:5])\n",
    "    \n",
    "    # Now try to load just the model safely\n",
    "    model = ChangeClassifier(weights=None)\n",
    "    \n",
    "    # Manually filter out any problematic tensors if needed\n",
    "    safe_state_dict = {}\n",
    "    for key, value in checkpoint['model_state_dict'].items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            safe_state_dict[key] = value\n",
    "        else:\n",
    "            print(f\"Non-tensor found in state_dict: {key} - {type(value)}\")\n",
    "            # Convert non-tensors to tensors if possible\n",
    "            if hasattr(value, '__array__'):\n",
    "                safe_state_dict[key] = torch.tensor(np.array(value))\n",
    "    \n",
    "    model.load_state_dict(safe_state_dict, strict=False)\n",
    "    model.eval()\n",
    "    torch.save(model.state_dict(), \"../pretrained_models/model_entire.pth\")\n",
    "    print(\"Success!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56c0c8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from models.change_classifier import ChangeClassifier\n",
    "\n",
    "# Load the clean model file (safe with weights_only=True)\n",
    "model = ChangeClassifier(weights=None)\n",
    "model.load_state_dict(torch.load(\"../pretrained_models/model_entire.pth\", weights_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bbc5c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Forward pass OK\n",
      "Test shape: torch.Size([1, 3, 256, 256])\n",
      "Ref shape: torch.Size([1, 3, 256, 256])\n",
      "Output shape: torch.Size([1, 1, 256, 256])\n",
      "Total Parameters: 285,803\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.instancenorm.InstanceNorm2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_prelu() for <class 'torch.nn.modules.activation.PReLU'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "FLOPs: 1.633G\n",
      "THOP Params: 285.803K\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from thop import profile, clever_format\n",
    "from models.change_classifier import ChangeClassifier\n",
    "\n",
    "# Dummy input\n",
    "ref = torch.randn(1, 3, 256, 256)\n",
    "test = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# Model init\n",
    "model = ChangeClassifier(\n",
    "    weights=None,\n",
    "    output_layer_bkbn=\"3\",\n",
    "    freeze_backbone=False\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Forward pass test\n",
    "with torch.no_grad():\n",
    "    out = model(ref, test)\n",
    "\n",
    "print(\"✅ Forward pass OK\")\n",
    "print(\"Test shape:\", test.shape)\n",
    "print(\"Ref shape:\", ref.shape)\n",
    "\n",
    "print(\"Output shape:\", out.shape)\n",
    "\n",
    "# Params count\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "\n",
    "# FLOPs + MACs\n",
    "flops, params = profile(model, inputs=(ref, test))\n",
    "flops, params = clever_format([flops, params], \"%.3f\")\n",
    "print(f\"FLOPs: {flops}\")\n",
    "print(f\"THOP Params: {params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "011933f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Forward pass OK\n",
      "Test shape: torch.Size([1, 3, 256, 256])\n",
      "Ref shape: torch.Size([1, 3, 256, 256])\n",
      "Output shape: torch.Size([1, 1, 256, 256])\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.instancenorm.InstanceNorm2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_prelu() for <class 'torch.nn.modules.activation.PReLU'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "⚙️ FLOPs: 1.633G\n",
      "📦 THOP Params: 285.803K\n",
      "🔍 Full Model Structure:\n",
      "\n",
      "ChangeClassifier(\n",
      "  (_retina): RetinaSimBlock(\n",
      "    (dog): Conv2d(3, 3, kernel_size=(15, 15), stride=(1, 1), padding=(7, 7), groups=3, bias=False)\n",
      "    (adapt): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (act): Tanh()\n",
      "  )\n",
      "  (_backbone): ModuleList(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            (0): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.00625, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.018750000000000003, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.03125, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n",
      "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.043750000000000004, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n",
      "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n",
      "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05625, mode=row)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_first_mix): MixingMaskAttentionBlock(\n",
      "    (_mixing): MixingBlock(\n",
      "      (_convmix): Sequential(\n",
      "        (0): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3)\n",
      "        (1): PReLU(num_parameters=1)\n",
      "        (2): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (_linear): PixelwiseLinear(\n",
      "      (_linears): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(3, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): PReLU(num_parameters=1)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(10, 5, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): PReLU(num_parameters=1)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(5, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): PReLU(num_parameters=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_mixing_mask): ModuleList(\n",
      "    (0): MixingMaskAttentionBlock(\n",
      "      (_mixing): MixingBlock(\n",
      "        (_convmix): Sequential(\n",
      "          (0): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)\n",
      "          (1): PReLU(num_parameters=1)\n",
      "          (2): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        )\n",
      "      )\n",
      "      (_linear): PixelwiseLinear(\n",
      "        (_linears): Sequential(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): PReLU(num_parameters=1)\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv2d(12, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): PReLU(num_parameters=1)\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv2d(6, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): MixingMaskAttentionBlock(\n",
      "      (_mixing): MixingBlock(\n",
      "        (_convmix): Sequential(\n",
      "          (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "          (1): PReLU(num_parameters=1)\n",
      "          (2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        )\n",
      "      )\n",
      "      (_linear): PixelwiseLinear(\n",
      "        (_linears): Sequential(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): PReLU(num_parameters=1)\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): PReLU(num_parameters=1)\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): MixingBlock(\n",
      "      (_convmix): Sequential(\n",
      "        (0): Conv2d(112, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=56)\n",
      "        (1): PReLU(num_parameters=1)\n",
      "        (2): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_up): ModuleList(\n",
      "    (0): UpMask(\n",
      "      (_upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (_convolution): Sequential(\n",
      "        (0): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=56)\n",
      "        (1): PReLU(num_parameters=1)\n",
      "        (2): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): Conv2d(56, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): PReLU(num_parameters=1)\n",
      "        (5): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (1): UpMask(\n",
      "      (_upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (_convolution): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "        (1): PReLU(num_parameters=1)\n",
      "        (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): PReLU(num_parameters=1)\n",
      "        (5): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (2): UpMask(\n",
      "      (_upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (_convolution): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "        (1): PReLU(num_parameters=1)\n",
      "        (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): PReLU(num_parameters=1)\n",
      "        (5): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_classify): PixelwiseLinear(\n",
      "    (_linears): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from thop import profile, clever_format\n",
    "from models.change_classifier import ChangeClassifier\n",
    "\n",
    "ref = torch.randn(1, 3, 256, 256)\n",
    "test = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "model = ChangeClassifier(\n",
    "    weights=None,\n",
    "    output_layer_bkbn=\"3\",\n",
    "    freeze_backbone=False\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "# Forward test\n",
    "with torch.no_grad():\n",
    "    out = model(ref, test)\n",
    "\n",
    "print(\"✅ Forward pass OK\")\n",
    "print(\"Test shape:\", test.shape)\n",
    "print(\"Ref shape:\", ref.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "\n",
    "# Param count\n",
    "#total_params = sum(p.numel() for p in model.parameters())\n",
    "#print(f\"🧠 Total Parameters: {total_params:,}\")\n",
    "\n",
    "# FLOPs + MACs\n",
    "flops, params = profile(model, inputs=(ref, test))\n",
    "flops, params = clever_format([flops, params], \"%.3f\")\n",
    "print(f\"⚙️ FLOPs: {flops}\")\n",
    "print(f\"📦 THOP Params: {params}\")\n",
    "\n",
    "# ✅ Print the full model architecture\n",
    "print(\"🔍 Full Model Structure:\\n\")\n",
    "print(model)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a907c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_graph.png'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "# example with dummy input (make sure inputs match your model)\n",
    "t1_input = torch.randn(1, 3, 256, 256)\n",
    "t2_input = torch.randn(1, 3, 256, 256)\n",
    "out = model(t1_input, t2_input)\n",
    "# Create graph\n",
    "dot = make_dot(out, params=dict(model.named_parameters()))\n",
    "# Save to file\n",
    "dot.format = 'png'\n",
    "dot.render('../pretrained_models/model_graph')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_input = torch.randn(1, 3, 256, 256)\n",
    "t2_input = torch.randn(1, 3, 256, 256)\n",
    "model.eval()\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (t1_input, t2_input),  # <--- pack as a tuple\n",
    "    \"../pretrained_models/model.onnx\",\n",
    "    opset_version=11,\n",
    "    input_names=[\"img1\", \"img2\"],\n",
    "    output_names=[\"pred\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae84e230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_retina\n",
      "_retina.dog\n",
      "_retina.adapt\n",
      "_retina.act\n",
      "_backbone\n",
      "_backbone.0\n",
      "_backbone.0.0\n",
      "_backbone.0.1\n",
      "_backbone.0.2\n",
      "_backbone.1\n",
      "_backbone.1.0\n",
      "_backbone.1.0.block\n",
      "_backbone.1.0.block.0\n",
      "_backbone.1.0.block.0.0\n",
      "_backbone.1.0.block.0.1\n",
      "_backbone.1.0.block.0.2\n",
      "_backbone.1.0.block.1\n",
      "_backbone.1.0.block.1.avgpool\n",
      "_backbone.1.0.block.1.fc1\n",
      "_backbone.1.0.block.1.fc2\n",
      "_backbone.1.0.block.1.activation\n",
      "_backbone.1.0.block.1.scale_activation\n",
      "_backbone.1.0.block.2\n",
      "_backbone.1.0.block.2.0\n",
      "_backbone.1.0.block.2.1\n",
      "_backbone.1.0.stochastic_depth\n",
      "_backbone.1.1\n",
      "_backbone.1.1.block\n",
      "_backbone.1.1.block.0\n",
      "_backbone.1.1.block.0.0\n",
      "_backbone.1.1.block.0.1\n",
      "_backbone.1.1.block.0.2\n",
      "_backbone.1.1.block.1\n",
      "_backbone.1.1.block.1.avgpool\n",
      "_backbone.1.1.block.1.fc1\n",
      "_backbone.1.1.block.1.fc2\n",
      "_backbone.1.1.block.1.activation\n",
      "_backbone.1.1.block.1.scale_activation\n",
      "_backbone.1.1.block.2\n",
      "_backbone.1.1.block.2.0\n",
      "_backbone.1.1.block.2.1\n",
      "_backbone.1.1.stochastic_depth\n",
      "_backbone.2\n",
      "_backbone.2.0\n",
      "_backbone.2.0.block\n",
      "_backbone.2.0.block.0\n",
      "_backbone.2.0.block.0.0\n",
      "_backbone.2.0.block.0.1\n",
      "_backbone.2.0.block.0.2\n",
      "_backbone.2.0.block.1\n",
      "_backbone.2.0.block.1.0\n",
      "_backbone.2.0.block.1.1\n",
      "_backbone.2.0.block.1.2\n",
      "_backbone.2.0.block.2\n",
      "_backbone.2.0.block.2.avgpool\n",
      "_backbone.2.0.block.2.fc1\n",
      "_backbone.2.0.block.2.fc2\n",
      "_backbone.2.0.block.2.activation\n",
      "_backbone.2.0.block.2.scale_activation\n",
      "_backbone.2.0.block.3\n",
      "_backbone.2.0.block.3.0\n",
      "_backbone.2.0.block.3.1\n",
      "_backbone.2.0.stochastic_depth\n",
      "_backbone.2.1\n",
      "_backbone.2.1.block\n",
      "_backbone.2.1.block.0\n",
      "_backbone.2.1.block.0.0\n",
      "_backbone.2.1.block.0.1\n",
      "_backbone.2.1.block.0.2\n",
      "_backbone.2.1.block.1\n",
      "_backbone.2.1.block.1.0\n",
      "_backbone.2.1.block.1.1\n",
      "_backbone.2.1.block.1.2\n",
      "_backbone.2.1.block.2\n",
      "_backbone.2.1.block.2.avgpool\n",
      "_backbone.2.1.block.2.fc1\n",
      "_backbone.2.1.block.2.fc2\n",
      "_backbone.2.1.block.2.activation\n",
      "_backbone.2.1.block.2.scale_activation\n",
      "_backbone.2.1.block.3\n",
      "_backbone.2.1.block.3.0\n",
      "_backbone.2.1.block.3.1\n",
      "_backbone.2.1.stochastic_depth\n",
      "_backbone.2.2\n",
      "_backbone.2.2.block\n",
      "_backbone.2.2.block.0\n",
      "_backbone.2.2.block.0.0\n",
      "_backbone.2.2.block.0.1\n",
      "_backbone.2.2.block.0.2\n",
      "_backbone.2.2.block.1\n",
      "_backbone.2.2.block.1.0\n",
      "_backbone.2.2.block.1.1\n",
      "_backbone.2.2.block.1.2\n",
      "_backbone.2.2.block.2\n",
      "_backbone.2.2.block.2.avgpool\n",
      "_backbone.2.2.block.2.fc1\n",
      "_backbone.2.2.block.2.fc2\n",
      "_backbone.2.2.block.2.activation\n",
      "_backbone.2.2.block.2.scale_activation\n",
      "_backbone.2.2.block.3\n",
      "_backbone.2.2.block.3.0\n",
      "_backbone.2.2.block.3.1\n",
      "_backbone.2.2.stochastic_depth\n",
      "_backbone.2.3\n",
      "_backbone.2.3.block\n",
      "_backbone.2.3.block.0\n",
      "_backbone.2.3.block.0.0\n",
      "_backbone.2.3.block.0.1\n",
      "_backbone.2.3.block.0.2\n",
      "_backbone.2.3.block.1\n",
      "_backbone.2.3.block.1.0\n",
      "_backbone.2.3.block.1.1\n",
      "_backbone.2.3.block.1.2\n",
      "_backbone.2.3.block.2\n",
      "_backbone.2.3.block.2.avgpool\n",
      "_backbone.2.3.block.2.fc1\n",
      "_backbone.2.3.block.2.fc2\n",
      "_backbone.2.3.block.2.activation\n",
      "_backbone.2.3.block.2.scale_activation\n",
      "_backbone.2.3.block.3\n",
      "_backbone.2.3.block.3.0\n",
      "_backbone.2.3.block.3.1\n",
      "_backbone.2.3.stochastic_depth\n",
      "_backbone.3\n",
      "_backbone.3.0\n",
      "_backbone.3.0.block\n",
      "_backbone.3.0.block.0\n",
      "_backbone.3.0.block.0.0\n",
      "_backbone.3.0.block.0.1\n",
      "_backbone.3.0.block.0.2\n",
      "_backbone.3.0.block.1\n",
      "_backbone.3.0.block.1.0\n",
      "_backbone.3.0.block.1.1\n",
      "_backbone.3.0.block.1.2\n",
      "_backbone.3.0.block.2\n",
      "_backbone.3.0.block.2.avgpool\n",
      "_backbone.3.0.block.2.fc1\n",
      "_backbone.3.0.block.2.fc2\n",
      "_backbone.3.0.block.2.activation\n",
      "_backbone.3.0.block.2.scale_activation\n",
      "_backbone.3.0.block.3\n",
      "_backbone.3.0.block.3.0\n",
      "_backbone.3.0.block.3.1\n",
      "_backbone.3.0.stochastic_depth\n",
      "_backbone.3.1\n",
      "_backbone.3.1.block\n",
      "_backbone.3.1.block.0\n",
      "_backbone.3.1.block.0.0\n",
      "_backbone.3.1.block.0.1\n",
      "_backbone.3.1.block.0.2\n",
      "_backbone.3.1.block.1\n",
      "_backbone.3.1.block.1.0\n",
      "_backbone.3.1.block.1.1\n",
      "_backbone.3.1.block.1.2\n",
      "_backbone.3.1.block.2\n",
      "_backbone.3.1.block.2.avgpool\n",
      "_backbone.3.1.block.2.fc1\n",
      "_backbone.3.1.block.2.fc2\n",
      "_backbone.3.1.block.2.activation\n",
      "_backbone.3.1.block.2.scale_activation\n",
      "_backbone.3.1.block.3\n",
      "_backbone.3.1.block.3.0\n",
      "_backbone.3.1.block.3.1\n",
      "_backbone.3.1.stochastic_depth\n",
      "_backbone.3.2\n",
      "_backbone.3.2.block\n",
      "_backbone.3.2.block.0\n",
      "_backbone.3.2.block.0.0\n",
      "_backbone.3.2.block.0.1\n",
      "_backbone.3.2.block.0.2\n",
      "_backbone.3.2.block.1\n",
      "_backbone.3.2.block.1.0\n",
      "_backbone.3.2.block.1.1\n",
      "_backbone.3.2.block.1.2\n",
      "_backbone.3.2.block.2\n",
      "_backbone.3.2.block.2.avgpool\n",
      "_backbone.3.2.block.2.fc1\n",
      "_backbone.3.2.block.2.fc2\n",
      "_backbone.3.2.block.2.activation\n",
      "_backbone.3.2.block.2.scale_activation\n",
      "_backbone.3.2.block.3\n",
      "_backbone.3.2.block.3.0\n",
      "_backbone.3.2.block.3.1\n",
      "_backbone.3.2.stochastic_depth\n",
      "_backbone.3.3\n",
      "_backbone.3.3.block\n",
      "_backbone.3.3.block.0\n",
      "_backbone.3.3.block.0.0\n",
      "_backbone.3.3.block.0.1\n",
      "_backbone.3.3.block.0.2\n",
      "_backbone.3.3.block.1\n",
      "_backbone.3.3.block.1.0\n",
      "_backbone.3.3.block.1.1\n",
      "_backbone.3.3.block.1.2\n",
      "_backbone.3.3.block.2\n",
      "_backbone.3.3.block.2.avgpool\n",
      "_backbone.3.3.block.2.fc1\n",
      "_backbone.3.3.block.2.fc2\n",
      "_backbone.3.3.block.2.activation\n",
      "_backbone.3.3.block.2.scale_activation\n",
      "_backbone.3.3.block.3\n",
      "_backbone.3.3.block.3.0\n",
      "_backbone.3.3.block.3.1\n",
      "_backbone.3.3.stochastic_depth\n",
      "_first_mix\n",
      "_first_mix._mixing\n",
      "_first_mix._mixing._convmix\n",
      "_first_mix._mixing._convmix.0\n",
      "_first_mix._mixing._convmix.1\n",
      "_first_mix._mixing._convmix.2\n",
      "_first_mix._linear\n",
      "_first_mix._linear._linears\n",
      "_first_mix._linear._linears.0\n",
      "_first_mix._linear._linears.0.0\n",
      "_first_mix._linear._linears.0.1\n",
      "_first_mix._linear._linears.1\n",
      "_first_mix._linear._linears.1.0\n",
      "_first_mix._linear._linears.1.1\n",
      "_first_mix._linear._linears.2\n",
      "_first_mix._linear._linears.2.0\n",
      "_first_mix._linear._linears.2.1\n",
      "_mixing_mask\n",
      "_mixing_mask.0\n",
      "_mixing_mask.0._mixing\n",
      "_mixing_mask.0._mixing._convmix\n",
      "_mixing_mask.0._mixing._convmix.0\n",
      "_mixing_mask.0._mixing._convmix.1\n",
      "_mixing_mask.0._mixing._convmix.2\n",
      "_mixing_mask.0._linear\n",
      "_mixing_mask.0._linear._linears\n",
      "_mixing_mask.0._linear._linears.0\n",
      "_mixing_mask.0._linear._linears.0.0\n",
      "_mixing_mask.0._linear._linears.0.1\n",
      "_mixing_mask.0._linear._linears.1\n",
      "_mixing_mask.0._linear._linears.1.0\n",
      "_mixing_mask.0._linear._linears.1.1\n",
      "_mixing_mask.0._linear._linears.2\n",
      "_mixing_mask.0._linear._linears.2.0\n",
      "_mixing_mask.0._linear._linears.2.1\n",
      "_mixing_mask.1\n",
      "_mixing_mask.1._mixing\n",
      "_mixing_mask.1._mixing._convmix\n",
      "_mixing_mask.1._mixing._convmix.0\n",
      "_mixing_mask.1._mixing._convmix.1\n",
      "_mixing_mask.1._mixing._convmix.2\n",
      "_mixing_mask.1._linear\n",
      "_mixing_mask.1._linear._linears\n",
      "_mixing_mask.1._linear._linears.0\n",
      "_mixing_mask.1._linear._linears.0.0\n",
      "_mixing_mask.1._linear._linears.0.1\n",
      "_mixing_mask.1._linear._linears.1\n",
      "_mixing_mask.1._linear._linears.1.0\n",
      "_mixing_mask.1._linear._linears.1.1\n",
      "_mixing_mask.1._linear._linears.2\n",
      "_mixing_mask.1._linear._linears.2.0\n",
      "_mixing_mask.1._linear._linears.2.1\n",
      "_mixing_mask.2\n",
      "_mixing_mask.2._convmix\n",
      "_mixing_mask.2._convmix.0\n",
      "_mixing_mask.2._convmix.1\n",
      "_mixing_mask.2._convmix.2\n",
      "_up\n",
      "_up.0\n",
      "_up.0._upsample\n",
      "_up.0._convolution\n",
      "_up.0._convolution.0\n",
      "_up.0._convolution.1\n",
      "_up.0._convolution.2\n",
      "_up.0._convolution.3\n",
      "_up.0._convolution.4\n",
      "_up.0._convolution.5\n",
      "_up.1\n",
      "_up.1._upsample\n",
      "_up.1._convolution\n",
      "_up.1._convolution.0\n",
      "_up.1._convolution.1\n",
      "_up.1._convolution.2\n",
      "_up.1._convolution.3\n",
      "_up.1._convolution.4\n",
      "_up.1._convolution.5\n",
      "_up.2\n",
      "_up.2._upsample\n",
      "_up.2._convolution\n",
      "_up.2._convolution.0\n",
      "_up.2._convolution.1\n",
      "_up.2._convolution.2\n",
      "_up.2._convolution.3\n",
      "_up.2._convolution.4\n",
      "_up.2._convolution.5\n",
      "_classify\n",
      "_classify._linears\n",
      "_classify._linears.0\n",
      "_classify._linears.0.0\n",
      "_classify._linears.0.1\n",
      "_classify._linears.1\n",
      "_classify._linears.1.0\n",
      "_classify._linears.1.1\n",
      "_classify._linears.2\n",
      "_classify._linears.2.0\n",
      "_classify._linears.2.1\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c401ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_retina.dog.weight: torch.Size([3, 1, 15, 15])\n",
      "_backbone.0.0.weight: torch.Size([48, 3, 3, 3])\n",
      "_backbone.0.1.weight: torch.Size([48])\n",
      "_backbone.0.1.bias: torch.Size([48])\n",
      "_backbone.0.1.running_mean: torch.Size([48])\n",
      "_backbone.0.1.running_var: torch.Size([48])\n",
      "_backbone.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.1.0.block.0.0.weight: torch.Size([48, 1, 3, 3])\n",
      "_backbone.1.0.block.0.1.weight: torch.Size([48])\n",
      "_backbone.1.0.block.0.1.bias: torch.Size([48])\n",
      "_backbone.1.0.block.0.1.running_mean: torch.Size([48])\n",
      "_backbone.1.0.block.0.1.running_var: torch.Size([48])\n",
      "_backbone.1.0.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.1.0.block.1.fc1.weight: torch.Size([12, 48, 1, 1])\n",
      "_backbone.1.0.block.1.fc1.bias: torch.Size([12])\n",
      "_backbone.1.0.block.1.fc2.weight: torch.Size([48, 12, 1, 1])\n",
      "_backbone.1.0.block.1.fc2.bias: torch.Size([48])\n",
      "_backbone.1.0.block.2.0.weight: torch.Size([24, 48, 1, 1])\n",
      "_backbone.1.0.block.2.1.weight: torch.Size([24])\n",
      "_backbone.1.0.block.2.1.bias: torch.Size([24])\n",
      "_backbone.1.0.block.2.1.running_mean: torch.Size([24])\n",
      "_backbone.1.0.block.2.1.running_var: torch.Size([24])\n",
      "_backbone.1.0.block.2.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.1.1.block.0.0.weight: torch.Size([24, 1, 3, 3])\n",
      "_backbone.1.1.block.0.1.weight: torch.Size([24])\n",
      "_backbone.1.1.block.0.1.bias: torch.Size([24])\n",
      "_backbone.1.1.block.0.1.running_mean: torch.Size([24])\n",
      "_backbone.1.1.block.0.1.running_var: torch.Size([24])\n",
      "_backbone.1.1.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.1.1.block.1.fc1.weight: torch.Size([6, 24, 1, 1])\n",
      "_backbone.1.1.block.1.fc1.bias: torch.Size([6])\n",
      "_backbone.1.1.block.1.fc2.weight: torch.Size([24, 6, 1, 1])\n",
      "_backbone.1.1.block.1.fc2.bias: torch.Size([24])\n",
      "_backbone.1.1.block.2.0.weight: torch.Size([24, 24, 1, 1])\n",
      "_backbone.1.1.block.2.1.weight: torch.Size([24])\n",
      "_backbone.1.1.block.2.1.bias: torch.Size([24])\n",
      "_backbone.1.1.block.2.1.running_mean: torch.Size([24])\n",
      "_backbone.1.1.block.2.1.running_var: torch.Size([24])\n",
      "_backbone.1.1.block.2.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.0.block.0.0.weight: torch.Size([144, 24, 1, 1])\n",
      "_backbone.2.0.block.0.1.weight: torch.Size([144])\n",
      "_backbone.2.0.block.0.1.bias: torch.Size([144])\n",
      "_backbone.2.0.block.0.1.running_mean: torch.Size([144])\n",
      "_backbone.2.0.block.0.1.running_var: torch.Size([144])\n",
      "_backbone.2.0.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.0.block.1.0.weight: torch.Size([144, 1, 3, 3])\n",
      "_backbone.2.0.block.1.1.weight: torch.Size([144])\n",
      "_backbone.2.0.block.1.1.bias: torch.Size([144])\n",
      "_backbone.2.0.block.1.1.running_mean: torch.Size([144])\n",
      "_backbone.2.0.block.1.1.running_var: torch.Size([144])\n",
      "_backbone.2.0.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.0.block.2.fc1.weight: torch.Size([6, 144, 1, 1])\n",
      "_backbone.2.0.block.2.fc1.bias: torch.Size([6])\n",
      "_backbone.2.0.block.2.fc2.weight: torch.Size([144, 6, 1, 1])\n",
      "_backbone.2.0.block.2.fc2.bias: torch.Size([144])\n",
      "_backbone.2.0.block.3.0.weight: torch.Size([32, 144, 1, 1])\n",
      "_backbone.2.0.block.3.1.weight: torch.Size([32])\n",
      "_backbone.2.0.block.3.1.bias: torch.Size([32])\n",
      "_backbone.2.0.block.3.1.running_mean: torch.Size([32])\n",
      "_backbone.2.0.block.3.1.running_var: torch.Size([32])\n",
      "_backbone.2.0.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.1.block.0.0.weight: torch.Size([192, 32, 1, 1])\n",
      "_backbone.2.1.block.0.1.weight: torch.Size([192])\n",
      "_backbone.2.1.block.0.1.bias: torch.Size([192])\n",
      "_backbone.2.1.block.0.1.running_mean: torch.Size([192])\n",
      "_backbone.2.1.block.0.1.running_var: torch.Size([192])\n",
      "_backbone.2.1.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.1.block.1.0.weight: torch.Size([192, 1, 3, 3])\n",
      "_backbone.2.1.block.1.1.weight: torch.Size([192])\n",
      "_backbone.2.1.block.1.1.bias: torch.Size([192])\n",
      "_backbone.2.1.block.1.1.running_mean: torch.Size([192])\n",
      "_backbone.2.1.block.1.1.running_var: torch.Size([192])\n",
      "_backbone.2.1.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.1.block.2.fc1.weight: torch.Size([8, 192, 1, 1])\n",
      "_backbone.2.1.block.2.fc1.bias: torch.Size([8])\n",
      "_backbone.2.1.block.2.fc2.weight: torch.Size([192, 8, 1, 1])\n",
      "_backbone.2.1.block.2.fc2.bias: torch.Size([192])\n",
      "_backbone.2.1.block.3.0.weight: torch.Size([32, 192, 1, 1])\n",
      "_backbone.2.1.block.3.1.weight: torch.Size([32])\n",
      "_backbone.2.1.block.3.1.bias: torch.Size([32])\n",
      "_backbone.2.1.block.3.1.running_mean: torch.Size([32])\n",
      "_backbone.2.1.block.3.1.running_var: torch.Size([32])\n",
      "_backbone.2.1.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.2.block.0.0.weight: torch.Size([192, 32, 1, 1])\n",
      "_backbone.2.2.block.0.1.weight: torch.Size([192])\n",
      "_backbone.2.2.block.0.1.bias: torch.Size([192])\n",
      "_backbone.2.2.block.0.1.running_mean: torch.Size([192])\n",
      "_backbone.2.2.block.0.1.running_var: torch.Size([192])\n",
      "_backbone.2.2.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.2.block.1.0.weight: torch.Size([192, 1, 3, 3])\n",
      "_backbone.2.2.block.1.1.weight: torch.Size([192])\n",
      "_backbone.2.2.block.1.1.bias: torch.Size([192])\n",
      "_backbone.2.2.block.1.1.running_mean: torch.Size([192])\n",
      "_backbone.2.2.block.1.1.running_var: torch.Size([192])\n",
      "_backbone.2.2.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.2.block.2.fc1.weight: torch.Size([8, 192, 1, 1])\n",
      "_backbone.2.2.block.2.fc1.bias: torch.Size([8])\n",
      "_backbone.2.2.block.2.fc2.weight: torch.Size([192, 8, 1, 1])\n",
      "_backbone.2.2.block.2.fc2.bias: torch.Size([192])\n",
      "_backbone.2.2.block.3.0.weight: torch.Size([32, 192, 1, 1])\n",
      "_backbone.2.2.block.3.1.weight: torch.Size([32])\n",
      "_backbone.2.2.block.3.1.bias: torch.Size([32])\n",
      "_backbone.2.2.block.3.1.running_mean: torch.Size([32])\n",
      "_backbone.2.2.block.3.1.running_var: torch.Size([32])\n",
      "_backbone.2.2.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.3.block.0.0.weight: torch.Size([192, 32, 1, 1])\n",
      "_backbone.2.3.block.0.1.weight: torch.Size([192])\n",
      "_backbone.2.3.block.0.1.bias: torch.Size([192])\n",
      "_backbone.2.3.block.0.1.running_mean: torch.Size([192])\n",
      "_backbone.2.3.block.0.1.running_var: torch.Size([192])\n",
      "_backbone.2.3.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.3.block.1.0.weight: torch.Size([192, 1, 3, 3])\n",
      "_backbone.2.3.block.1.1.weight: torch.Size([192])\n",
      "_backbone.2.3.block.1.1.bias: torch.Size([192])\n",
      "_backbone.2.3.block.1.1.running_mean: torch.Size([192])\n",
      "_backbone.2.3.block.1.1.running_var: torch.Size([192])\n",
      "_backbone.2.3.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.3.block.2.fc1.weight: torch.Size([8, 192, 1, 1])\n",
      "_backbone.2.3.block.2.fc1.bias: torch.Size([8])\n",
      "_backbone.2.3.block.2.fc2.weight: torch.Size([192, 8, 1, 1])\n",
      "_backbone.2.3.block.2.fc2.bias: torch.Size([192])\n",
      "_backbone.2.3.block.3.0.weight: torch.Size([32, 192, 1, 1])\n",
      "_backbone.2.3.block.3.1.weight: torch.Size([32])\n",
      "_backbone.2.3.block.3.1.bias: torch.Size([32])\n",
      "_backbone.2.3.block.3.1.running_mean: torch.Size([32])\n",
      "_backbone.2.3.block.3.1.running_var: torch.Size([32])\n",
      "_backbone.2.3.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.0.block.0.0.weight: torch.Size([192, 32, 1, 1])\n",
      "_backbone.3.0.block.0.1.weight: torch.Size([192])\n",
      "_backbone.3.0.block.0.1.bias: torch.Size([192])\n",
      "_backbone.3.0.block.0.1.running_mean: torch.Size([192])\n",
      "_backbone.3.0.block.0.1.running_var: torch.Size([192])\n",
      "_backbone.3.0.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.0.block.1.0.weight: torch.Size([192, 1, 5, 5])\n",
      "_backbone.3.0.block.1.1.weight: torch.Size([192])\n",
      "_backbone.3.0.block.1.1.bias: torch.Size([192])\n",
      "_backbone.3.0.block.1.1.running_mean: torch.Size([192])\n",
      "_backbone.3.0.block.1.1.running_var: torch.Size([192])\n",
      "_backbone.3.0.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.0.block.2.fc1.weight: torch.Size([8, 192, 1, 1])\n",
      "_backbone.3.0.block.2.fc1.bias: torch.Size([8])\n",
      "_backbone.3.0.block.2.fc2.weight: torch.Size([192, 8, 1, 1])\n",
      "_backbone.3.0.block.2.fc2.bias: torch.Size([192])\n",
      "_backbone.3.0.block.3.0.weight: torch.Size([56, 192, 1, 1])\n",
      "_backbone.3.0.block.3.1.weight: torch.Size([56])\n",
      "_backbone.3.0.block.3.1.bias: torch.Size([56])\n",
      "_backbone.3.0.block.3.1.running_mean: torch.Size([56])\n",
      "_backbone.3.0.block.3.1.running_var: torch.Size([56])\n",
      "_backbone.3.0.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.1.block.0.0.weight: torch.Size([336, 56, 1, 1])\n",
      "_backbone.3.1.block.0.1.weight: torch.Size([336])\n",
      "_backbone.3.1.block.0.1.bias: torch.Size([336])\n",
      "_backbone.3.1.block.0.1.running_mean: torch.Size([336])\n",
      "_backbone.3.1.block.0.1.running_var: torch.Size([336])\n",
      "_backbone.3.1.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.1.block.1.0.weight: torch.Size([336, 1, 5, 5])\n",
      "_backbone.3.1.block.1.1.weight: torch.Size([336])\n",
      "_backbone.3.1.block.1.1.bias: torch.Size([336])\n",
      "_backbone.3.1.block.1.1.running_mean: torch.Size([336])\n",
      "_backbone.3.1.block.1.1.running_var: torch.Size([336])\n",
      "_backbone.3.1.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.1.block.2.fc1.weight: torch.Size([14, 336, 1, 1])\n",
      "_backbone.3.1.block.2.fc1.bias: torch.Size([14])\n",
      "_backbone.3.1.block.2.fc2.weight: torch.Size([336, 14, 1, 1])\n",
      "_backbone.3.1.block.2.fc2.bias: torch.Size([336])\n",
      "_backbone.3.1.block.3.0.weight: torch.Size([56, 336, 1, 1])\n",
      "_backbone.3.1.block.3.1.weight: torch.Size([56])\n",
      "_backbone.3.1.block.3.1.bias: torch.Size([56])\n",
      "_backbone.3.1.block.3.1.running_mean: torch.Size([56])\n",
      "_backbone.3.1.block.3.1.running_var: torch.Size([56])\n",
      "_backbone.3.1.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.2.block.0.0.weight: torch.Size([336, 56, 1, 1])\n",
      "_backbone.3.2.block.0.1.weight: torch.Size([336])\n",
      "_backbone.3.2.block.0.1.bias: torch.Size([336])\n",
      "_backbone.3.2.block.0.1.running_mean: torch.Size([336])\n",
      "_backbone.3.2.block.0.1.running_var: torch.Size([336])\n",
      "_backbone.3.2.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.2.block.1.0.weight: torch.Size([336, 1, 5, 5])\n",
      "_backbone.3.2.block.1.1.weight: torch.Size([336])\n",
      "_backbone.3.2.block.1.1.bias: torch.Size([336])\n",
      "_backbone.3.2.block.1.1.running_mean: torch.Size([336])\n",
      "_backbone.3.2.block.1.1.running_var: torch.Size([336])\n",
      "_backbone.3.2.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.2.block.2.fc1.weight: torch.Size([14, 336, 1, 1])\n",
      "_backbone.3.2.block.2.fc1.bias: torch.Size([14])\n",
      "_backbone.3.2.block.2.fc2.weight: torch.Size([336, 14, 1, 1])\n",
      "_backbone.3.2.block.2.fc2.bias: torch.Size([336])\n",
      "_backbone.3.2.block.3.0.weight: torch.Size([56, 336, 1, 1])\n",
      "_backbone.3.2.block.3.1.weight: torch.Size([56])\n",
      "_backbone.3.2.block.3.1.bias: torch.Size([56])\n",
      "_backbone.3.2.block.3.1.running_mean: torch.Size([56])\n",
      "_backbone.3.2.block.3.1.running_var: torch.Size([56])\n",
      "_backbone.3.2.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.3.block.0.0.weight: torch.Size([336, 56, 1, 1])\n",
      "_backbone.3.3.block.0.1.weight: torch.Size([336])\n",
      "_backbone.3.3.block.0.1.bias: torch.Size([336])\n",
      "_backbone.3.3.block.0.1.running_mean: torch.Size([336])\n",
      "_backbone.3.3.block.0.1.running_var: torch.Size([336])\n",
      "_backbone.3.3.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.3.block.1.0.weight: torch.Size([336, 1, 5, 5])\n",
      "_backbone.3.3.block.1.1.weight: torch.Size([336])\n",
      "_backbone.3.3.block.1.1.bias: torch.Size([336])\n",
      "_backbone.3.3.block.1.1.running_mean: torch.Size([336])\n",
      "_backbone.3.3.block.1.1.running_var: torch.Size([336])\n",
      "_backbone.3.3.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.3.block.2.fc1.weight: torch.Size([14, 336, 1, 1])\n",
      "_backbone.3.3.block.2.fc1.bias: torch.Size([14])\n",
      "_backbone.3.3.block.2.fc2.weight: torch.Size([336, 14, 1, 1])\n",
      "_backbone.3.3.block.2.fc2.bias: torch.Size([336])\n",
      "_backbone.3.3.block.3.0.weight: torch.Size([56, 336, 1, 1])\n",
      "_backbone.3.3.block.3.1.weight: torch.Size([56])\n",
      "_backbone.3.3.block.3.1.bias: torch.Size([56])\n",
      "_backbone.3.3.block.3.1.running_mean: torch.Size([56])\n",
      "_backbone.3.3.block.3.1.running_var: torch.Size([56])\n",
      "_backbone.3.3.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_first_mix._mixing._convmix.0.weight: torch.Size([3, 2, 3, 3])\n",
      "_first_mix._mixing._convmix.0.bias: torch.Size([3])\n",
      "_first_mix._mixing._convmix.1.weight: torch.Size([1])\n",
      "_first_mix._linear._linears.0.0.weight: torch.Size([10, 3, 1, 1])\n",
      "_first_mix._linear._linears.0.0.bias: torch.Size([10])\n",
      "_first_mix._linear._linears.0.1.weight: torch.Size([1])\n",
      "_first_mix._linear._linears.1.0.weight: torch.Size([5, 10, 1, 1])\n",
      "_first_mix._linear._linears.1.0.bias: torch.Size([5])\n",
      "_first_mix._linear._linears.1.1.weight: torch.Size([1])\n",
      "_first_mix._linear._linears.2.0.weight: torch.Size([1, 5, 1, 1])\n",
      "_first_mix._linear._linears.2.0.bias: torch.Size([1])\n",
      "_first_mix._linear._linears.2.1.weight: torch.Size([1])\n",
      "_mixing_mask.0._mixing._convmix.0.weight: torch.Size([24, 2, 3, 3])\n",
      "_mixing_mask.0._mixing._convmix.0.bias: torch.Size([24])\n",
      "_mixing_mask.0._mixing._convmix.1.weight: torch.Size([1])\n",
      "_mixing_mask.0._linear._linears.0.0.weight: torch.Size([12, 24, 1, 1])\n",
      "_mixing_mask.0._linear._linears.0.0.bias: torch.Size([12])\n",
      "_mixing_mask.0._linear._linears.0.1.weight: torch.Size([1])\n",
      "_mixing_mask.0._linear._linears.1.0.weight: torch.Size([6, 12, 1, 1])\n",
      "_mixing_mask.0._linear._linears.1.0.bias: torch.Size([6])\n",
      "_mixing_mask.0._linear._linears.1.1.weight: torch.Size([1])\n",
      "_mixing_mask.0._linear._linears.2.0.weight: torch.Size([1, 6, 1, 1])\n",
      "_mixing_mask.0._linear._linears.2.0.bias: torch.Size([1])\n",
      "_mixing_mask.0._linear._linears.2.1.weight: torch.Size([1])\n",
      "_mixing_mask.1._mixing._convmix.0.weight: torch.Size([32, 2, 3, 3])\n",
      "_mixing_mask.1._mixing._convmix.0.bias: torch.Size([32])\n",
      "_mixing_mask.1._mixing._convmix.1.weight: torch.Size([1])\n",
      "_mixing_mask.1._linear._linears.0.0.weight: torch.Size([16, 32, 1, 1])\n",
      "_mixing_mask.1._linear._linears.0.0.bias: torch.Size([16])\n",
      "_mixing_mask.1._linear._linears.0.1.weight: torch.Size([1])\n",
      "_mixing_mask.1._linear._linears.1.0.weight: torch.Size([8, 16, 1, 1])\n",
      "_mixing_mask.1._linear._linears.1.0.bias: torch.Size([8])\n",
      "_mixing_mask.1._linear._linears.1.1.weight: torch.Size([1])\n",
      "_mixing_mask.1._linear._linears.2.0.weight: torch.Size([1, 8, 1, 1])\n",
      "_mixing_mask.1._linear._linears.2.0.bias: torch.Size([1])\n",
      "_mixing_mask.1._linear._linears.2.1.weight: torch.Size([1])\n",
      "_mixing_mask.2._convmix.0.weight: torch.Size([56, 2, 3, 3])\n",
      "_mixing_mask.2._convmix.0.bias: torch.Size([56])\n",
      "_mixing_mask.2._convmix.1.weight: torch.Size([1])\n",
      "_up.0._convolution.0.weight: torch.Size([56, 1, 3, 3])\n",
      "_up.0._convolution.0.bias: torch.Size([56])\n",
      "_up.0._convolution.1.weight: torch.Size([1])\n",
      "_up.0._convolution.3.weight: torch.Size([64, 56, 1, 1])\n",
      "_up.0._convolution.3.bias: torch.Size([64])\n",
      "_up.0._convolution.4.weight: torch.Size([1])\n",
      "_up.1._convolution.0.weight: torch.Size([64, 1, 3, 3])\n",
      "_up.1._convolution.0.bias: torch.Size([64])\n",
      "_up.1._convolution.1.weight: torch.Size([1])\n",
      "_up.1._convolution.3.weight: torch.Size([64, 64, 1, 1])\n",
      "_up.1._convolution.3.bias: torch.Size([64])\n",
      "_up.1._convolution.4.weight: torch.Size([1])\n",
      "_up.2._convolution.0.weight: torch.Size([64, 1, 3, 3])\n",
      "_up.2._convolution.0.bias: torch.Size([64])\n",
      "_up.2._convolution.1.weight: torch.Size([1])\n",
      "_up.2._convolution.3.weight: torch.Size([32, 64, 1, 1])\n",
      "_up.2._convolution.3.bias: torch.Size([32])\n",
      "_up.2._convolution.4.weight: torch.Size([1])\n",
      "_classify._linears.0.0.weight: torch.Size([16, 32, 1, 1])\n",
      "_classify._linears.0.0.bias: torch.Size([16])\n",
      "_classify._linears.0.1.weight: torch.Size([1])\n",
      "_classify._linears.1.0.weight: torch.Size([8, 16, 1, 1])\n",
      "_classify._linears.1.0.bias: torch.Size([8])\n",
      "_classify._linears.1.1.weight: torch.Size([1])\n",
      "_classify._linears.2.0.weight: torch.Size([1, 8, 1, 1])\n",
      "_classify._linears.2.0.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(\"../pretrained_models/model_entire.pth\", weights_only=True)\n",
    "\n",
    "for k, v in ckpt.items():\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ece4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
