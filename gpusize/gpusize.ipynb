{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad330148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import torch\n",
    "from models.change_classifier import ChangeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e24f7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRECISE MEMORY CALCULATION ===\n",
      "--- Feature Map Memory Breakdown ---\n",
      "input: 256x256x3 = 18.0 MB\n",
      "backbone.0: 128x128x48 = 72.0 MB\n",
      "backbone.1: 64x64x24 = 9.0 MB\n",
      "backbone.2: 32x32x32 = 3.0 MB\n",
      "backbone.3: 16x16x56 = 1.3 MB\n",
      "mixing_mask.2: 16x16x56 = 1.3 MB\n",
      "up.0: 32x32x64 = 6.0 MB\n",
      "up.1: 64x64x64 = 24.0 MB\n",
      "up.2: 128x128x32 = 48.0 MB\n",
      "output: 256x256x1 = 6.0 MB\n",
      "\n",
      "--- PRECISE Memory Breakdown ---\n",
      "Parameters: 1.1 MB\n",
      "Input data: 42.0 MB\n",
      "Activations (TOTAL): 188.6 MB\n",
      "Gradients: 1.1 MB\n",
      "Optimizer: 2.2 MB\n",
      "PyTorch overhead (x3.0): 705.0 MB\n",
      "TOTAL: 705.0 MB (0.7 GB)\n",
      "\n",
      "--- Comparison with Reality ---\n",
      "Your actual usage: 13.6 GB\n",
      "My calculation: 0.7 GB\n",
      "Difference: 12.9 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "def precise_memory_calculation(batch_size=24, image_size=256):\n",
    "    \"\"\"\n",
    "    PRECISE memory calculation based on actual layer dimensions\n",
    "    \"\"\"\n",
    "    print(\"=== PRECISE MEMORY CALCULATION ===\")\n",
    "    \n",
    "    # Based on your state_dict, here are the critical layer dimensions:\n",
    "    # This shows the feature map progression through the network\n",
    "    \n",
    "    # Feature map dimensions at each stage (estimated from layer shapes)\n",
    "    feature_map_progression = [\n",
    "        # (height, width, channels, layer_type)\n",
    "        (256, 256, 3, \"input\"),\n",
    "        (128, 128, 48, \"backbone.0\"),      # After first conv\n",
    "        (64, 64, 24, \"backbone.1\"),        # After MBConv blocks\n",
    "        (32, 32, 32, \"backbone.2\"),        # More MBConv blocks  \n",
    "        (16, 16, 56, \"backbone.3\"),        # Final backbone output\n",
    "        (16, 16, 56, \"mixing_mask.2\"),     # After mixing\n",
    "        (32, 32, 64, \"up.0\"),              # After first upsample\n",
    "        (64, 64, 64, \"up.1\"),              # After second upsample\n",
    "        (128, 128, 32, \"up.2\"),            # After third upsample\n",
    "        (256, 256, 1, \"output\")            # Final output\n",
    "    ]\n",
    "    \n",
    "    bytes_per_float = 4\n",
    "    total_activation_memory = 0\n",
    "    \n",
    "    print(\"--- Feature Map Memory Breakdown ---\")\n",
    "    for i, (h, w, c, layer_name) in enumerate(feature_map_progression):\n",
    "        layer_memory = batch_size * h * w * c * bytes_per_float / (1024 * 1024)\n",
    "        total_activation_memory += layer_memory\n",
    "        print(f\"{layer_name}: {h}x{w}x{c} = {layer_memory:.1f} MB\")\n",
    "    \n",
    "    # Parameter memory (from your state_dict - I can count them)\n",
    "    total_params = 285803  # From your earlier output\n",
    "    param_memory_mb = total_params * bytes_per_float / (1024 * 1024)\n",
    "    \n",
    "    # Input data memory\n",
    "    input_memory_mb = batch_size * (2 * 256 * 256 * 3 + 256 * 256 * 1) * bytes_per_float / (1024 * 1024)\n",
    "    \n",
    "    # Gradients and optimizer\n",
    "    gradients_memory_mb = param_memory_mb\n",
    "    optimizer_memory_mb = 2 * param_memory_mb\n",
    "    \n",
    "    # PyTorch overhead (CUDA context, fragmentation, etc.)\n",
    "    pytorch_overhead = 3.0  # Higher factor for complex models\n",
    "    \n",
    "    total_memory_mb = (\n",
    "        param_memory_mb + \n",
    "        input_memory_mb + \n",
    "        total_activation_memory + \n",
    "        gradients_memory_mb + \n",
    "        optimizer_memory_mb\n",
    "    ) * pytorch_overhead\n",
    "    \n",
    "    print(f\"\\n--- PRECISE Memory Breakdown ---\")\n",
    "    print(f\"Parameters: {param_memory_mb:.1f} MB\")\n",
    "    print(f\"Input data: {input_memory_mb:.1f} MB\")\n",
    "    print(f\"Activations (TOTAL): {total_activation_memory:.1f} MB\")\n",
    "    print(f\"Gradients: {gradients_memory_mb:.1f} MB\")\n",
    "    print(f\"Optimizer: {optimizer_memory_mb:.1f} MB\")\n",
    "    print(f\"PyTorch overhead (x{pytorch_overhead}): {total_memory_mb:.1f} MB\")\n",
    "    print(f\"TOTAL: {total_memory_mb:.1f} MB ({total_memory_mb/1024:.1f} GB)\")\n",
    "    \n",
    "    # Compare with your actual usage\n",
    "    print(f\"\\n--- Comparison with Reality ---\")\n",
    "    print(f\"Your actual usage: 13.6 GB\")\n",
    "    print(f\"My calculation: {total_memory_mb/1024:.1f} GB\")\n",
    "    print(f\"Difference: {abs(13.6 - total_memory_mb/1024):.1f} GB\")\n",
    "    \n",
    "    return total_memory_mb\n",
    "\n",
    "# Run the precise calculation\n",
    "if __name__ == \"__main__\":\n",
    "    precise_memory_calculation(batch_size=24, image_size=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
