{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a03aad",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options \n\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39madd_safe_globals([scalar])\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m ChangeClassifier(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../pretrained_models/checkpoint_086.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])   \u001b[38;5;66;03m# <- key bit\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:1096\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1090\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1091\u001b[0m                              map_location,\n\u001b[0;32m   1092\u001b[0m                              _weights_only_unpickler,\n\u001b[0;32m   1093\u001b[0m                              overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1094\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1095\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1096\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1098\u001b[0m             opened_zipfile,\n\u001b[0;32m   1099\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1102\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1103\u001b[0m         )\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options \n\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import torch\n",
    "from numpy.core.multiarray import scalar\n",
    "import torch\n",
    "from models.change_classifier import ChangeClassifier\n",
    "\n",
    "torch.serialization.add_safe_globals([scalar])\n",
    "\n",
    "model = ChangeClassifier(weights=None)\n",
    "ckpt = torch.load(\"../pretrained_models/checkpoint_086.pth\", pickle_module=pickle, weights_only=True)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])   # <- key bit\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), \"../pretrained_models/model_entire.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bbc5c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Forward pass OK\n",
      "Test shape: torch.Size([1, 3, 256, 256])\n",
      "Ref shape: torch.Size([1, 3, 256, 256])\n",
      "Output shape: torch.Size([1, 1, 256, 256])\n",
      "Total Parameters: 285,803\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.instancenorm.InstanceNorm2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_prelu() for <class 'torch.nn.modules.activation.PReLU'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCC\\AppData\\Roaming\\Python\\Python311\\site-packages\\thop\\vision\\calc_func.py:53: UserWarning: This API is being deprecated\n",
      "  warnings.warn(\"This API is being deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 1.633G\n",
      "THOP Params: 285.803K\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from thop import profile, clever_format\n",
    "from models.change_classifier import ChangeClassifier\n",
    "\n",
    "# Dummy input\n",
    "ref = torch.randn(1, 3, 256, 256)\n",
    "test = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# Model init\n",
    "model = ChangeClassifier(\n",
    "    bkbn_name=\"efficientnet_b4\",\n",
    "    weights=None,\n",
    "    output_layer_bkbn=\"3\",\n",
    "    freeze_backbone=False\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Forward pass test\n",
    "with torch.no_grad():\n",
    "    out = model(ref, test)\n",
    "\n",
    "print(\"‚úÖ Forward pass OK\")\n",
    "print(\"Test shape:\", test.shape)\n",
    "print(\"Ref shape:\", ref.shape)\n",
    "\n",
    "print(\"Output shape:\", out.shape)\n",
    "\n",
    "# Params count\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "\n",
    "# FLOPs + MACs\n",
    "flops, params = profile(model, inputs=(ref, test))\n",
    "flops, params = clever_format([flops, params], \"%.3f\")\n",
    "print(f\"FLOPs: {flops}\")\n",
    "print(f\"THOP Params: {params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "011933f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Forward pass OK\n",
      "Test shape: torch.Size([1, 3, 256, 256])\n",
      "Ref shape: torch.Size([1, 3, 256, 256])\n",
      "Output shape: torch.Size([1, 1, 256, 256])\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.instancenorm.InstanceNorm2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_prelu() for <class 'torch.nn.modules.activation.PReLU'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "‚öôÔ∏è FLOPs: 1.633G\n",
      "üì¶ THOP Params: 285.803K\n",
      "üîç Full Model Structure:\n",
      "\n",
      "ChangeClassifier(\n",
      "  (_retina): RetinaSimBlock(\n",
      "    (dog): Conv2d(3, 3, kernel_size=(15, 15), stride=(1, 1), padding=(7, 7), groups=3, bias=False)\n",
      "    (adapt): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (act): Tanh()\n",
      "  )\n",
      "  (_backbone): ModuleList(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            (0): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.00625, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.018750000000000003, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.03125, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n",
      "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.043750000000000004, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n",
      "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n",
      "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05625, mode=row)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_first_mix): MixingMaskAttentionBlock(\n",
      "    (_mixing): MixingBlock(\n",
      "      (_convmix): Sequential(\n",
      "        (0): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3)\n",
      "        (1): PReLU(num_parameters=1)\n",
      "        (2): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (_linear): PixelwiseLinear(\n",
      "      (_linears): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(3, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): PReLU(num_parameters=1)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(10, 5, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): PReLU(num_parameters=1)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(5, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): PReLU(num_parameters=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_mixing_mask): ModuleList(\n",
      "    (0): MixingMaskAttentionBlock(\n",
      "      (_mixing): MixingBlock(\n",
      "        (_convmix): Sequential(\n",
      "          (0): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)\n",
      "          (1): PReLU(num_parameters=1)\n",
      "          (2): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        )\n",
      "      )\n",
      "      (_linear): PixelwiseLinear(\n",
      "        (_linears): Sequential(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): PReLU(num_parameters=1)\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv2d(12, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): PReLU(num_parameters=1)\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv2d(6, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): MixingMaskAttentionBlock(\n",
      "      (_mixing): MixingBlock(\n",
      "        (_convmix): Sequential(\n",
      "          (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "          (1): PReLU(num_parameters=1)\n",
      "          (2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        )\n",
      "      )\n",
      "      (_linear): PixelwiseLinear(\n",
      "        (_linears): Sequential(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): PReLU(num_parameters=1)\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): PReLU(num_parameters=1)\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): MixingBlock(\n",
      "      (_convmix): Sequential(\n",
      "        (0): Conv2d(112, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=56)\n",
      "        (1): PReLU(num_parameters=1)\n",
      "        (2): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_up): ModuleList(\n",
      "    (0): UpMask(\n",
      "      (_upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (_convolution): Sequential(\n",
      "        (0): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=56)\n",
      "        (1): PReLU(num_parameters=1)\n",
      "        (2): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): Conv2d(56, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): PReLU(num_parameters=1)\n",
      "        (5): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (1): UpMask(\n",
      "      (_upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (_convolution): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "        (1): PReLU(num_parameters=1)\n",
      "        (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): PReLU(num_parameters=1)\n",
      "        (5): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (2): UpMask(\n",
      "      (_upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      (_convolution): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "        (1): PReLU(num_parameters=1)\n",
      "        (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): PReLU(num_parameters=1)\n",
      "        (5): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_classify): PixelwiseLinear(\n",
      "    (_linears): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from thop import profile, clever_format\n",
    "from models.change_classifier import ChangeClassifier\n",
    "\n",
    "ref = torch.randn(1, 3, 256, 256)\n",
    "test = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "model = ChangeClassifier(\n",
    "    bkbn_name=\"efficientnet_b4\",\n",
    "    weights=None,\n",
    "    output_layer_bkbn=\"3\",\n",
    "    freeze_backbone=False\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "# Forward test\n",
    "with torch.no_grad():\n",
    "    out = model(ref, test)\n",
    "\n",
    "print(\"‚úÖ Forward pass OK\")\n",
    "print(\"Test shape:\", test.shape)\n",
    "print(\"Ref shape:\", ref.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "\n",
    "# Param count\n",
    "#total_params = sum(p.numel() for p in model.parameters())\n",
    "#print(f\"üß† Total Parameters: {total_params:,}\")\n",
    "\n",
    "# FLOPs + MACs\n",
    "flops, params = profile(model, inputs=(ref, test))\n",
    "flops, params = clever_format([flops, params], \"%.3f\")\n",
    "print(f\"‚öôÔ∏è FLOPs: {flops}\")\n",
    "print(f\"üì¶ THOP Params: {params}\")\n",
    "\n",
    "# ‚úÖ Print the full model architecture\n",
    "print(\"üîç Full Model Structure:\\n\")\n",
    "print(model)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a907c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_graph.png'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "# example with dummy input (make sure inputs match your model)\n",
    "t1_input = torch.randn(1, 3, 256, 256)\n",
    "t2_input = torch.randn(1, 3, 256, 256)\n",
    "out = model(t1_input, t2_input)\n",
    "# Create graph\n",
    "dot = make_dot(out, params=dict(model.named_parameters()))\n",
    "# Save to file\n",
    "dot.format = 'png'\n",
    "dot.render('../pretrained_models/model_graph')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_input = torch.randn(1, 3, 256, 256)\n",
    "t2_input = torch.randn(1, 3, 256, 256)\n",
    "model.eval()\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (t1_input, t2_input),  # <--- pack as a tuple\n",
    "    \"../pretrained_models/model.onnx\",\n",
    "    opset_version=11,\n",
    "    input_names=[\"img1\", \"img2\"],\n",
    "    output_names=[\"pred\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae84e230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_retina\n",
      "_retina.dog\n",
      "_retina.adapt\n",
      "_retina.act\n",
      "_backbone\n",
      "_backbone.0\n",
      "_backbone.0.0\n",
      "_backbone.0.1\n",
      "_backbone.0.2\n",
      "_backbone.1\n",
      "_backbone.1.0\n",
      "_backbone.1.0.block\n",
      "_backbone.1.0.block.0\n",
      "_backbone.1.0.block.0.0\n",
      "_backbone.1.0.block.0.1\n",
      "_backbone.1.0.block.0.2\n",
      "_backbone.1.0.block.1\n",
      "_backbone.1.0.block.1.avgpool\n",
      "_backbone.1.0.block.1.fc1\n",
      "_backbone.1.0.block.1.fc2\n",
      "_backbone.1.0.block.1.activation\n",
      "_backbone.1.0.block.1.scale_activation\n",
      "_backbone.1.0.block.2\n",
      "_backbone.1.0.block.2.0\n",
      "_backbone.1.0.block.2.1\n",
      "_backbone.1.0.stochastic_depth\n",
      "_backbone.1.1\n",
      "_backbone.1.1.block\n",
      "_backbone.1.1.block.0\n",
      "_backbone.1.1.block.0.0\n",
      "_backbone.1.1.block.0.1\n",
      "_backbone.1.1.block.0.2\n",
      "_backbone.1.1.block.1\n",
      "_backbone.1.1.block.1.avgpool\n",
      "_backbone.1.1.block.1.fc1\n",
      "_backbone.1.1.block.1.fc2\n",
      "_backbone.1.1.block.1.activation\n",
      "_backbone.1.1.block.1.scale_activation\n",
      "_backbone.1.1.block.2\n",
      "_backbone.1.1.block.2.0\n",
      "_backbone.1.1.block.2.1\n",
      "_backbone.1.1.stochastic_depth\n",
      "_backbone.2\n",
      "_backbone.2.0\n",
      "_backbone.2.0.block\n",
      "_backbone.2.0.block.0\n",
      "_backbone.2.0.block.0.0\n",
      "_backbone.2.0.block.0.1\n",
      "_backbone.2.0.block.0.2\n",
      "_backbone.2.0.block.1\n",
      "_backbone.2.0.block.1.0\n",
      "_backbone.2.0.block.1.1\n",
      "_backbone.2.0.block.1.2\n",
      "_backbone.2.0.block.2\n",
      "_backbone.2.0.block.2.avgpool\n",
      "_backbone.2.0.block.2.fc1\n",
      "_backbone.2.0.block.2.fc2\n",
      "_backbone.2.0.block.2.activation\n",
      "_backbone.2.0.block.2.scale_activation\n",
      "_backbone.2.0.block.3\n",
      "_backbone.2.0.block.3.0\n",
      "_backbone.2.0.block.3.1\n",
      "_backbone.2.0.stochastic_depth\n",
      "_backbone.2.1\n",
      "_backbone.2.1.block\n",
      "_backbone.2.1.block.0\n",
      "_backbone.2.1.block.0.0\n",
      "_backbone.2.1.block.0.1\n",
      "_backbone.2.1.block.0.2\n",
      "_backbone.2.1.block.1\n",
      "_backbone.2.1.block.1.0\n",
      "_backbone.2.1.block.1.1\n",
      "_backbone.2.1.block.1.2\n",
      "_backbone.2.1.block.2\n",
      "_backbone.2.1.block.2.avgpool\n",
      "_backbone.2.1.block.2.fc1\n",
      "_backbone.2.1.block.2.fc2\n",
      "_backbone.2.1.block.2.activation\n",
      "_backbone.2.1.block.2.scale_activation\n",
      "_backbone.2.1.block.3\n",
      "_backbone.2.1.block.3.0\n",
      "_backbone.2.1.block.3.1\n",
      "_backbone.2.1.stochastic_depth\n",
      "_backbone.2.2\n",
      "_backbone.2.2.block\n",
      "_backbone.2.2.block.0\n",
      "_backbone.2.2.block.0.0\n",
      "_backbone.2.2.block.0.1\n",
      "_backbone.2.2.block.0.2\n",
      "_backbone.2.2.block.1\n",
      "_backbone.2.2.block.1.0\n",
      "_backbone.2.2.block.1.1\n",
      "_backbone.2.2.block.1.2\n",
      "_backbone.2.2.block.2\n",
      "_backbone.2.2.block.2.avgpool\n",
      "_backbone.2.2.block.2.fc1\n",
      "_backbone.2.2.block.2.fc2\n",
      "_backbone.2.2.block.2.activation\n",
      "_backbone.2.2.block.2.scale_activation\n",
      "_backbone.2.2.block.3\n",
      "_backbone.2.2.block.3.0\n",
      "_backbone.2.2.block.3.1\n",
      "_backbone.2.2.stochastic_depth\n",
      "_backbone.2.3\n",
      "_backbone.2.3.block\n",
      "_backbone.2.3.block.0\n",
      "_backbone.2.3.block.0.0\n",
      "_backbone.2.3.block.0.1\n",
      "_backbone.2.3.block.0.2\n",
      "_backbone.2.3.block.1\n",
      "_backbone.2.3.block.1.0\n",
      "_backbone.2.3.block.1.1\n",
      "_backbone.2.3.block.1.2\n",
      "_backbone.2.3.block.2\n",
      "_backbone.2.3.block.2.avgpool\n",
      "_backbone.2.3.block.2.fc1\n",
      "_backbone.2.3.block.2.fc2\n",
      "_backbone.2.3.block.2.activation\n",
      "_backbone.2.3.block.2.scale_activation\n",
      "_backbone.2.3.block.3\n",
      "_backbone.2.3.block.3.0\n",
      "_backbone.2.3.block.3.1\n",
      "_backbone.2.3.stochastic_depth\n",
      "_backbone.3\n",
      "_backbone.3.0\n",
      "_backbone.3.0.block\n",
      "_backbone.3.0.block.0\n",
      "_backbone.3.0.block.0.0\n",
      "_backbone.3.0.block.0.1\n",
      "_backbone.3.0.block.0.2\n",
      "_backbone.3.0.block.1\n",
      "_backbone.3.0.block.1.0\n",
      "_backbone.3.0.block.1.1\n",
      "_backbone.3.0.block.1.2\n",
      "_backbone.3.0.block.2\n",
      "_backbone.3.0.block.2.avgpool\n",
      "_backbone.3.0.block.2.fc1\n",
      "_backbone.3.0.block.2.fc2\n",
      "_backbone.3.0.block.2.activation\n",
      "_backbone.3.0.block.2.scale_activation\n",
      "_backbone.3.0.block.3\n",
      "_backbone.3.0.block.3.0\n",
      "_backbone.3.0.block.3.1\n",
      "_backbone.3.0.stochastic_depth\n",
      "_backbone.3.1\n",
      "_backbone.3.1.block\n",
      "_backbone.3.1.block.0\n",
      "_backbone.3.1.block.0.0\n",
      "_backbone.3.1.block.0.1\n",
      "_backbone.3.1.block.0.2\n",
      "_backbone.3.1.block.1\n",
      "_backbone.3.1.block.1.0\n",
      "_backbone.3.1.block.1.1\n",
      "_backbone.3.1.block.1.2\n",
      "_backbone.3.1.block.2\n",
      "_backbone.3.1.block.2.avgpool\n",
      "_backbone.3.1.block.2.fc1\n",
      "_backbone.3.1.block.2.fc2\n",
      "_backbone.3.1.block.2.activation\n",
      "_backbone.3.1.block.2.scale_activation\n",
      "_backbone.3.1.block.3\n",
      "_backbone.3.1.block.3.0\n",
      "_backbone.3.1.block.3.1\n",
      "_backbone.3.1.stochastic_depth\n",
      "_backbone.3.2\n",
      "_backbone.3.2.block\n",
      "_backbone.3.2.block.0\n",
      "_backbone.3.2.block.0.0\n",
      "_backbone.3.2.block.0.1\n",
      "_backbone.3.2.block.0.2\n",
      "_backbone.3.2.block.1\n",
      "_backbone.3.2.block.1.0\n",
      "_backbone.3.2.block.1.1\n",
      "_backbone.3.2.block.1.2\n",
      "_backbone.3.2.block.2\n",
      "_backbone.3.2.block.2.avgpool\n",
      "_backbone.3.2.block.2.fc1\n",
      "_backbone.3.2.block.2.fc2\n",
      "_backbone.3.2.block.2.activation\n",
      "_backbone.3.2.block.2.scale_activation\n",
      "_backbone.3.2.block.3\n",
      "_backbone.3.2.block.3.0\n",
      "_backbone.3.2.block.3.1\n",
      "_backbone.3.2.stochastic_depth\n",
      "_backbone.3.3\n",
      "_backbone.3.3.block\n",
      "_backbone.3.3.block.0\n",
      "_backbone.3.3.block.0.0\n",
      "_backbone.3.3.block.0.1\n",
      "_backbone.3.3.block.0.2\n",
      "_backbone.3.3.block.1\n",
      "_backbone.3.3.block.1.0\n",
      "_backbone.3.3.block.1.1\n",
      "_backbone.3.3.block.1.2\n",
      "_backbone.3.3.block.2\n",
      "_backbone.3.3.block.2.avgpool\n",
      "_backbone.3.3.block.2.fc1\n",
      "_backbone.3.3.block.2.fc2\n",
      "_backbone.3.3.block.2.activation\n",
      "_backbone.3.3.block.2.scale_activation\n",
      "_backbone.3.3.block.3\n",
      "_backbone.3.3.block.3.0\n",
      "_backbone.3.3.block.3.1\n",
      "_backbone.3.3.stochastic_depth\n",
      "_first_mix\n",
      "_first_mix._mixing\n",
      "_first_mix._mixing._convmix\n",
      "_first_mix._mixing._convmix.0\n",
      "_first_mix._mixing._convmix.1\n",
      "_first_mix._mixing._convmix.2\n",
      "_first_mix._linear\n",
      "_first_mix._linear._linears\n",
      "_first_mix._linear._linears.0\n",
      "_first_mix._linear._linears.0.0\n",
      "_first_mix._linear._linears.0.1\n",
      "_first_mix._linear._linears.1\n",
      "_first_mix._linear._linears.1.0\n",
      "_first_mix._linear._linears.1.1\n",
      "_first_mix._linear._linears.2\n",
      "_first_mix._linear._linears.2.0\n",
      "_first_mix._linear._linears.2.1\n",
      "_mixing_mask\n",
      "_mixing_mask.0\n",
      "_mixing_mask.0._mixing\n",
      "_mixing_mask.0._mixing._convmix\n",
      "_mixing_mask.0._mixing._convmix.0\n",
      "_mixing_mask.0._mixing._convmix.1\n",
      "_mixing_mask.0._mixing._convmix.2\n",
      "_mixing_mask.0._linear\n",
      "_mixing_mask.0._linear._linears\n",
      "_mixing_mask.0._linear._linears.0\n",
      "_mixing_mask.0._linear._linears.0.0\n",
      "_mixing_mask.0._linear._linears.0.1\n",
      "_mixing_mask.0._linear._linears.1\n",
      "_mixing_mask.0._linear._linears.1.0\n",
      "_mixing_mask.0._linear._linears.1.1\n",
      "_mixing_mask.0._linear._linears.2\n",
      "_mixing_mask.0._linear._linears.2.0\n",
      "_mixing_mask.0._linear._linears.2.1\n",
      "_mixing_mask.1\n",
      "_mixing_mask.1._mixing\n",
      "_mixing_mask.1._mixing._convmix\n",
      "_mixing_mask.1._mixing._convmix.0\n",
      "_mixing_mask.1._mixing._convmix.1\n",
      "_mixing_mask.1._mixing._convmix.2\n",
      "_mixing_mask.1._linear\n",
      "_mixing_mask.1._linear._linears\n",
      "_mixing_mask.1._linear._linears.0\n",
      "_mixing_mask.1._linear._linears.0.0\n",
      "_mixing_mask.1._linear._linears.0.1\n",
      "_mixing_mask.1._linear._linears.1\n",
      "_mixing_mask.1._linear._linears.1.0\n",
      "_mixing_mask.1._linear._linears.1.1\n",
      "_mixing_mask.1._linear._linears.2\n",
      "_mixing_mask.1._linear._linears.2.0\n",
      "_mixing_mask.1._linear._linears.2.1\n",
      "_mixing_mask.2\n",
      "_mixing_mask.2._convmix\n",
      "_mixing_mask.2._convmix.0\n",
      "_mixing_mask.2._convmix.1\n",
      "_mixing_mask.2._convmix.2\n",
      "_up\n",
      "_up.0\n",
      "_up.0._upsample\n",
      "_up.0._convolution\n",
      "_up.0._convolution.0\n",
      "_up.0._convolution.1\n",
      "_up.0._convolution.2\n",
      "_up.0._convolution.3\n",
      "_up.0._convolution.4\n",
      "_up.0._convolution.5\n",
      "_up.1\n",
      "_up.1._upsample\n",
      "_up.1._convolution\n",
      "_up.1._convolution.0\n",
      "_up.1._convolution.1\n",
      "_up.1._convolution.2\n",
      "_up.1._convolution.3\n",
      "_up.1._convolution.4\n",
      "_up.1._convolution.5\n",
      "_up.2\n",
      "_up.2._upsample\n",
      "_up.2._convolution\n",
      "_up.2._convolution.0\n",
      "_up.2._convolution.1\n",
      "_up.2._convolution.2\n",
      "_up.2._convolution.3\n",
      "_up.2._convolution.4\n",
      "_up.2._convolution.5\n",
      "_classify\n",
      "_classify._linears\n",
      "_classify._linears.0\n",
      "_classify._linears.0.0\n",
      "_classify._linears.0.1\n",
      "_classify._linears.1\n",
      "_classify._linears.1.0\n",
      "_classify._linears.1.1\n",
      "_classify._linears.2\n",
      "_classify._linears.2.0\n",
      "_classify._linears.2.1\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c401ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_retina.dog.weight: torch.Size([3, 1, 15, 15])\n",
      "_backbone.0.0.weight: torch.Size([48, 3, 3, 3])\n",
      "_backbone.0.1.weight: torch.Size([48])\n",
      "_backbone.0.1.bias: torch.Size([48])\n",
      "_backbone.0.1.running_mean: torch.Size([48])\n",
      "_backbone.0.1.running_var: torch.Size([48])\n",
      "_backbone.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.1.0.block.0.0.weight: torch.Size([48, 1, 3, 3])\n",
      "_backbone.1.0.block.0.1.weight: torch.Size([48])\n",
      "_backbone.1.0.block.0.1.bias: torch.Size([48])\n",
      "_backbone.1.0.block.0.1.running_mean: torch.Size([48])\n",
      "_backbone.1.0.block.0.1.running_var: torch.Size([48])\n",
      "_backbone.1.0.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.1.0.block.1.fc1.weight: torch.Size([12, 48, 1, 1])\n",
      "_backbone.1.0.block.1.fc1.bias: torch.Size([12])\n",
      "_backbone.1.0.block.1.fc2.weight: torch.Size([48, 12, 1, 1])\n",
      "_backbone.1.0.block.1.fc2.bias: torch.Size([48])\n",
      "_backbone.1.0.block.2.0.weight: torch.Size([24, 48, 1, 1])\n",
      "_backbone.1.0.block.2.1.weight: torch.Size([24])\n",
      "_backbone.1.0.block.2.1.bias: torch.Size([24])\n",
      "_backbone.1.0.block.2.1.running_mean: torch.Size([24])\n",
      "_backbone.1.0.block.2.1.running_var: torch.Size([24])\n",
      "_backbone.1.0.block.2.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.1.1.block.0.0.weight: torch.Size([24, 1, 3, 3])\n",
      "_backbone.1.1.block.0.1.weight: torch.Size([24])\n",
      "_backbone.1.1.block.0.1.bias: torch.Size([24])\n",
      "_backbone.1.1.block.0.1.running_mean: torch.Size([24])\n",
      "_backbone.1.1.block.0.1.running_var: torch.Size([24])\n",
      "_backbone.1.1.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.1.1.block.1.fc1.weight: torch.Size([6, 24, 1, 1])\n",
      "_backbone.1.1.block.1.fc1.bias: torch.Size([6])\n",
      "_backbone.1.1.block.1.fc2.weight: torch.Size([24, 6, 1, 1])\n",
      "_backbone.1.1.block.1.fc2.bias: torch.Size([24])\n",
      "_backbone.1.1.block.2.0.weight: torch.Size([24, 24, 1, 1])\n",
      "_backbone.1.1.block.2.1.weight: torch.Size([24])\n",
      "_backbone.1.1.block.2.1.bias: torch.Size([24])\n",
      "_backbone.1.1.block.2.1.running_mean: torch.Size([24])\n",
      "_backbone.1.1.block.2.1.running_var: torch.Size([24])\n",
      "_backbone.1.1.block.2.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.0.block.0.0.weight: torch.Size([144, 24, 1, 1])\n",
      "_backbone.2.0.block.0.1.weight: torch.Size([144])\n",
      "_backbone.2.0.block.0.1.bias: torch.Size([144])\n",
      "_backbone.2.0.block.0.1.running_mean: torch.Size([144])\n",
      "_backbone.2.0.block.0.1.running_var: torch.Size([144])\n",
      "_backbone.2.0.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.0.block.1.0.weight: torch.Size([144, 1, 3, 3])\n",
      "_backbone.2.0.block.1.1.weight: torch.Size([144])\n",
      "_backbone.2.0.block.1.1.bias: torch.Size([144])\n",
      "_backbone.2.0.block.1.1.running_mean: torch.Size([144])\n",
      "_backbone.2.0.block.1.1.running_var: torch.Size([144])\n",
      "_backbone.2.0.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.0.block.2.fc1.weight: torch.Size([6, 144, 1, 1])\n",
      "_backbone.2.0.block.2.fc1.bias: torch.Size([6])\n",
      "_backbone.2.0.block.2.fc2.weight: torch.Size([144, 6, 1, 1])\n",
      "_backbone.2.0.block.2.fc2.bias: torch.Size([144])\n",
      "_backbone.2.0.block.3.0.weight: torch.Size([32, 144, 1, 1])\n",
      "_backbone.2.0.block.3.1.weight: torch.Size([32])\n",
      "_backbone.2.0.block.3.1.bias: torch.Size([32])\n",
      "_backbone.2.0.block.3.1.running_mean: torch.Size([32])\n",
      "_backbone.2.0.block.3.1.running_var: torch.Size([32])\n",
      "_backbone.2.0.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.1.block.0.0.weight: torch.Size([192, 32, 1, 1])\n",
      "_backbone.2.1.block.0.1.weight: torch.Size([192])\n",
      "_backbone.2.1.block.0.1.bias: torch.Size([192])\n",
      "_backbone.2.1.block.0.1.running_mean: torch.Size([192])\n",
      "_backbone.2.1.block.0.1.running_var: torch.Size([192])\n",
      "_backbone.2.1.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.1.block.1.0.weight: torch.Size([192, 1, 3, 3])\n",
      "_backbone.2.1.block.1.1.weight: torch.Size([192])\n",
      "_backbone.2.1.block.1.1.bias: torch.Size([192])\n",
      "_backbone.2.1.block.1.1.running_mean: torch.Size([192])\n",
      "_backbone.2.1.block.1.1.running_var: torch.Size([192])\n",
      "_backbone.2.1.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.1.block.2.fc1.weight: torch.Size([8, 192, 1, 1])\n",
      "_backbone.2.1.block.2.fc1.bias: torch.Size([8])\n",
      "_backbone.2.1.block.2.fc2.weight: torch.Size([192, 8, 1, 1])\n",
      "_backbone.2.1.block.2.fc2.bias: torch.Size([192])\n",
      "_backbone.2.1.block.3.0.weight: torch.Size([32, 192, 1, 1])\n",
      "_backbone.2.1.block.3.1.weight: torch.Size([32])\n",
      "_backbone.2.1.block.3.1.bias: torch.Size([32])\n",
      "_backbone.2.1.block.3.1.running_mean: torch.Size([32])\n",
      "_backbone.2.1.block.3.1.running_var: torch.Size([32])\n",
      "_backbone.2.1.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.2.block.0.0.weight: torch.Size([192, 32, 1, 1])\n",
      "_backbone.2.2.block.0.1.weight: torch.Size([192])\n",
      "_backbone.2.2.block.0.1.bias: torch.Size([192])\n",
      "_backbone.2.2.block.0.1.running_mean: torch.Size([192])\n",
      "_backbone.2.2.block.0.1.running_var: torch.Size([192])\n",
      "_backbone.2.2.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.2.block.1.0.weight: torch.Size([192, 1, 3, 3])\n",
      "_backbone.2.2.block.1.1.weight: torch.Size([192])\n",
      "_backbone.2.2.block.1.1.bias: torch.Size([192])\n",
      "_backbone.2.2.block.1.1.running_mean: torch.Size([192])\n",
      "_backbone.2.2.block.1.1.running_var: torch.Size([192])\n",
      "_backbone.2.2.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.2.block.2.fc1.weight: torch.Size([8, 192, 1, 1])\n",
      "_backbone.2.2.block.2.fc1.bias: torch.Size([8])\n",
      "_backbone.2.2.block.2.fc2.weight: torch.Size([192, 8, 1, 1])\n",
      "_backbone.2.2.block.2.fc2.bias: torch.Size([192])\n",
      "_backbone.2.2.block.3.0.weight: torch.Size([32, 192, 1, 1])\n",
      "_backbone.2.2.block.3.1.weight: torch.Size([32])\n",
      "_backbone.2.2.block.3.1.bias: torch.Size([32])\n",
      "_backbone.2.2.block.3.1.running_mean: torch.Size([32])\n",
      "_backbone.2.2.block.3.1.running_var: torch.Size([32])\n",
      "_backbone.2.2.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.3.block.0.0.weight: torch.Size([192, 32, 1, 1])\n",
      "_backbone.2.3.block.0.1.weight: torch.Size([192])\n",
      "_backbone.2.3.block.0.1.bias: torch.Size([192])\n",
      "_backbone.2.3.block.0.1.running_mean: torch.Size([192])\n",
      "_backbone.2.3.block.0.1.running_var: torch.Size([192])\n",
      "_backbone.2.3.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.3.block.1.0.weight: torch.Size([192, 1, 3, 3])\n",
      "_backbone.2.3.block.1.1.weight: torch.Size([192])\n",
      "_backbone.2.3.block.1.1.bias: torch.Size([192])\n",
      "_backbone.2.3.block.1.1.running_mean: torch.Size([192])\n",
      "_backbone.2.3.block.1.1.running_var: torch.Size([192])\n",
      "_backbone.2.3.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.2.3.block.2.fc1.weight: torch.Size([8, 192, 1, 1])\n",
      "_backbone.2.3.block.2.fc1.bias: torch.Size([8])\n",
      "_backbone.2.3.block.2.fc2.weight: torch.Size([192, 8, 1, 1])\n",
      "_backbone.2.3.block.2.fc2.bias: torch.Size([192])\n",
      "_backbone.2.3.block.3.0.weight: torch.Size([32, 192, 1, 1])\n",
      "_backbone.2.3.block.3.1.weight: torch.Size([32])\n",
      "_backbone.2.3.block.3.1.bias: torch.Size([32])\n",
      "_backbone.2.3.block.3.1.running_mean: torch.Size([32])\n",
      "_backbone.2.3.block.3.1.running_var: torch.Size([32])\n",
      "_backbone.2.3.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.0.block.0.0.weight: torch.Size([192, 32, 1, 1])\n",
      "_backbone.3.0.block.0.1.weight: torch.Size([192])\n",
      "_backbone.3.0.block.0.1.bias: torch.Size([192])\n",
      "_backbone.3.0.block.0.1.running_mean: torch.Size([192])\n",
      "_backbone.3.0.block.0.1.running_var: torch.Size([192])\n",
      "_backbone.3.0.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.0.block.1.0.weight: torch.Size([192, 1, 5, 5])\n",
      "_backbone.3.0.block.1.1.weight: torch.Size([192])\n",
      "_backbone.3.0.block.1.1.bias: torch.Size([192])\n",
      "_backbone.3.0.block.1.1.running_mean: torch.Size([192])\n",
      "_backbone.3.0.block.1.1.running_var: torch.Size([192])\n",
      "_backbone.3.0.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.0.block.2.fc1.weight: torch.Size([8, 192, 1, 1])\n",
      "_backbone.3.0.block.2.fc1.bias: torch.Size([8])\n",
      "_backbone.3.0.block.2.fc2.weight: torch.Size([192, 8, 1, 1])\n",
      "_backbone.3.0.block.2.fc2.bias: torch.Size([192])\n",
      "_backbone.3.0.block.3.0.weight: torch.Size([56, 192, 1, 1])\n",
      "_backbone.3.0.block.3.1.weight: torch.Size([56])\n",
      "_backbone.3.0.block.3.1.bias: torch.Size([56])\n",
      "_backbone.3.0.block.3.1.running_mean: torch.Size([56])\n",
      "_backbone.3.0.block.3.1.running_var: torch.Size([56])\n",
      "_backbone.3.0.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.1.block.0.0.weight: torch.Size([336, 56, 1, 1])\n",
      "_backbone.3.1.block.0.1.weight: torch.Size([336])\n",
      "_backbone.3.1.block.0.1.bias: torch.Size([336])\n",
      "_backbone.3.1.block.0.1.running_mean: torch.Size([336])\n",
      "_backbone.3.1.block.0.1.running_var: torch.Size([336])\n",
      "_backbone.3.1.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.1.block.1.0.weight: torch.Size([336, 1, 5, 5])\n",
      "_backbone.3.1.block.1.1.weight: torch.Size([336])\n",
      "_backbone.3.1.block.1.1.bias: torch.Size([336])\n",
      "_backbone.3.1.block.1.1.running_mean: torch.Size([336])\n",
      "_backbone.3.1.block.1.1.running_var: torch.Size([336])\n",
      "_backbone.3.1.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.1.block.2.fc1.weight: torch.Size([14, 336, 1, 1])\n",
      "_backbone.3.1.block.2.fc1.bias: torch.Size([14])\n",
      "_backbone.3.1.block.2.fc2.weight: torch.Size([336, 14, 1, 1])\n",
      "_backbone.3.1.block.2.fc2.bias: torch.Size([336])\n",
      "_backbone.3.1.block.3.0.weight: torch.Size([56, 336, 1, 1])\n",
      "_backbone.3.1.block.3.1.weight: torch.Size([56])\n",
      "_backbone.3.1.block.3.1.bias: torch.Size([56])\n",
      "_backbone.3.1.block.3.1.running_mean: torch.Size([56])\n",
      "_backbone.3.1.block.3.1.running_var: torch.Size([56])\n",
      "_backbone.3.1.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.2.block.0.0.weight: torch.Size([336, 56, 1, 1])\n",
      "_backbone.3.2.block.0.1.weight: torch.Size([336])\n",
      "_backbone.3.2.block.0.1.bias: torch.Size([336])\n",
      "_backbone.3.2.block.0.1.running_mean: torch.Size([336])\n",
      "_backbone.3.2.block.0.1.running_var: torch.Size([336])\n",
      "_backbone.3.2.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.2.block.1.0.weight: torch.Size([336, 1, 5, 5])\n",
      "_backbone.3.2.block.1.1.weight: torch.Size([336])\n",
      "_backbone.3.2.block.1.1.bias: torch.Size([336])\n",
      "_backbone.3.2.block.1.1.running_mean: torch.Size([336])\n",
      "_backbone.3.2.block.1.1.running_var: torch.Size([336])\n",
      "_backbone.3.2.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.2.block.2.fc1.weight: torch.Size([14, 336, 1, 1])\n",
      "_backbone.3.2.block.2.fc1.bias: torch.Size([14])\n",
      "_backbone.3.2.block.2.fc2.weight: torch.Size([336, 14, 1, 1])\n",
      "_backbone.3.2.block.2.fc2.bias: torch.Size([336])\n",
      "_backbone.3.2.block.3.0.weight: torch.Size([56, 336, 1, 1])\n",
      "_backbone.3.2.block.3.1.weight: torch.Size([56])\n",
      "_backbone.3.2.block.3.1.bias: torch.Size([56])\n",
      "_backbone.3.2.block.3.1.running_mean: torch.Size([56])\n",
      "_backbone.3.2.block.3.1.running_var: torch.Size([56])\n",
      "_backbone.3.2.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.3.block.0.0.weight: torch.Size([336, 56, 1, 1])\n",
      "_backbone.3.3.block.0.1.weight: torch.Size([336])\n",
      "_backbone.3.3.block.0.1.bias: torch.Size([336])\n",
      "_backbone.3.3.block.0.1.running_mean: torch.Size([336])\n",
      "_backbone.3.3.block.0.1.running_var: torch.Size([336])\n",
      "_backbone.3.3.block.0.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.3.block.1.0.weight: torch.Size([336, 1, 5, 5])\n",
      "_backbone.3.3.block.1.1.weight: torch.Size([336])\n",
      "_backbone.3.3.block.1.1.bias: torch.Size([336])\n",
      "_backbone.3.3.block.1.1.running_mean: torch.Size([336])\n",
      "_backbone.3.3.block.1.1.running_var: torch.Size([336])\n",
      "_backbone.3.3.block.1.1.num_batches_tracked: torch.Size([])\n",
      "_backbone.3.3.block.2.fc1.weight: torch.Size([14, 336, 1, 1])\n",
      "_backbone.3.3.block.2.fc1.bias: torch.Size([14])\n",
      "_backbone.3.3.block.2.fc2.weight: torch.Size([336, 14, 1, 1])\n",
      "_backbone.3.3.block.2.fc2.bias: torch.Size([336])\n",
      "_backbone.3.3.block.3.0.weight: torch.Size([56, 336, 1, 1])\n",
      "_backbone.3.3.block.3.1.weight: torch.Size([56])\n",
      "_backbone.3.3.block.3.1.bias: torch.Size([56])\n",
      "_backbone.3.3.block.3.1.running_mean: torch.Size([56])\n",
      "_backbone.3.3.block.3.1.running_var: torch.Size([56])\n",
      "_backbone.3.3.block.3.1.num_batches_tracked: torch.Size([])\n",
      "_first_mix._mixing._convmix.0.weight: torch.Size([3, 2, 3, 3])\n",
      "_first_mix._mixing._convmix.0.bias: torch.Size([3])\n",
      "_first_mix._mixing._convmix.1.weight: torch.Size([1])\n",
      "_first_mix._linear._linears.0.0.weight: torch.Size([10, 3, 1, 1])\n",
      "_first_mix._linear._linears.0.0.bias: torch.Size([10])\n",
      "_first_mix._linear._linears.0.1.weight: torch.Size([1])\n",
      "_first_mix._linear._linears.1.0.weight: torch.Size([5, 10, 1, 1])\n",
      "_first_mix._linear._linears.1.0.bias: torch.Size([5])\n",
      "_first_mix._linear._linears.1.1.weight: torch.Size([1])\n",
      "_first_mix._linear._linears.2.0.weight: torch.Size([1, 5, 1, 1])\n",
      "_first_mix._linear._linears.2.0.bias: torch.Size([1])\n",
      "_first_mix._linear._linears.2.1.weight: torch.Size([1])\n",
      "_mixing_mask.0._mixing._convmix.0.weight: torch.Size([24, 2, 3, 3])\n",
      "_mixing_mask.0._mixing._convmix.0.bias: torch.Size([24])\n",
      "_mixing_mask.0._mixing._convmix.1.weight: torch.Size([1])\n",
      "_mixing_mask.0._linear._linears.0.0.weight: torch.Size([12, 24, 1, 1])\n",
      "_mixing_mask.0._linear._linears.0.0.bias: torch.Size([12])\n",
      "_mixing_mask.0._linear._linears.0.1.weight: torch.Size([1])\n",
      "_mixing_mask.0._linear._linears.1.0.weight: torch.Size([6, 12, 1, 1])\n",
      "_mixing_mask.0._linear._linears.1.0.bias: torch.Size([6])\n",
      "_mixing_mask.0._linear._linears.1.1.weight: torch.Size([1])\n",
      "_mixing_mask.0._linear._linears.2.0.weight: torch.Size([1, 6, 1, 1])\n",
      "_mixing_mask.0._linear._linears.2.0.bias: torch.Size([1])\n",
      "_mixing_mask.0._linear._linears.2.1.weight: torch.Size([1])\n",
      "_mixing_mask.1._mixing._convmix.0.weight: torch.Size([32, 2, 3, 3])\n",
      "_mixing_mask.1._mixing._convmix.0.bias: torch.Size([32])\n",
      "_mixing_mask.1._mixing._convmix.1.weight: torch.Size([1])\n",
      "_mixing_mask.1._linear._linears.0.0.weight: torch.Size([16, 32, 1, 1])\n",
      "_mixing_mask.1._linear._linears.0.0.bias: torch.Size([16])\n",
      "_mixing_mask.1._linear._linears.0.1.weight: torch.Size([1])\n",
      "_mixing_mask.1._linear._linears.1.0.weight: torch.Size([8, 16, 1, 1])\n",
      "_mixing_mask.1._linear._linears.1.0.bias: torch.Size([8])\n",
      "_mixing_mask.1._linear._linears.1.1.weight: torch.Size([1])\n",
      "_mixing_mask.1._linear._linears.2.0.weight: torch.Size([1, 8, 1, 1])\n",
      "_mixing_mask.1._linear._linears.2.0.bias: torch.Size([1])\n",
      "_mixing_mask.1._linear._linears.2.1.weight: torch.Size([1])\n",
      "_mixing_mask.2._convmix.0.weight: torch.Size([56, 2, 3, 3])\n",
      "_mixing_mask.2._convmix.0.bias: torch.Size([56])\n",
      "_mixing_mask.2._convmix.1.weight: torch.Size([1])\n",
      "_up.0._convolution.0.weight: torch.Size([56, 1, 3, 3])\n",
      "_up.0._convolution.0.bias: torch.Size([56])\n",
      "_up.0._convolution.1.weight: torch.Size([1])\n",
      "_up.0._convolution.3.weight: torch.Size([64, 56, 1, 1])\n",
      "_up.0._convolution.3.bias: torch.Size([64])\n",
      "_up.0._convolution.4.weight: torch.Size([1])\n",
      "_up.1._convolution.0.weight: torch.Size([64, 1, 3, 3])\n",
      "_up.1._convolution.0.bias: torch.Size([64])\n",
      "_up.1._convolution.1.weight: torch.Size([1])\n",
      "_up.1._convolution.3.weight: torch.Size([64, 64, 1, 1])\n",
      "_up.1._convolution.3.bias: torch.Size([64])\n",
      "_up.1._convolution.4.weight: torch.Size([1])\n",
      "_up.2._convolution.0.weight: torch.Size([64, 1, 3, 3])\n",
      "_up.2._convolution.0.bias: torch.Size([64])\n",
      "_up.2._convolution.1.weight: torch.Size([1])\n",
      "_up.2._convolution.3.weight: torch.Size([32, 64, 1, 1])\n",
      "_up.2._convolution.3.bias: torch.Size([32])\n",
      "_up.2._convolution.4.weight: torch.Size([1])\n",
      "_classify._linears.0.0.weight: torch.Size([16, 32, 1, 1])\n",
      "_classify._linears.0.0.bias: torch.Size([16])\n",
      "_classify._linears.0.1.weight: torch.Size([1])\n",
      "_classify._linears.1.0.weight: torch.Size([8, 16, 1, 1])\n",
      "_classify._linears.1.0.bias: torch.Size([8])\n",
      "_classify._linears.1.1.weight: torch.Size([1])\n",
      "_classify._linears.2.0.weight: torch.Size([1, 8, 1, 1])\n",
      "_classify._linears.2.0.bias: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCC\\AppData\\Local\\Temp\\ipykernel_2100\\3951728862.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"../pretrained_models/checkpoint_086.pth\", map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(\"../pretrained_models/checkpoint_086.pth\", map_location=\"cpu\")\n",
    "for k, v in ckpt[\"model_state_dict\"].items():\n",
    "    print(f\"{k}: {v.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91777759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCC\\AppData\\Local\\Temp\\ipykernel_2100\\1799161753.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"../pretrained_models/checkpoint_086.pth\")\n"
     ]
    }
   ],
   "source": [
    "from numpy.core.multiarray import scalar\n",
    "import torch\n",
    "\n",
    "torch.serialization.add_safe_globals([scalar])\n",
    "ckpt = torch.load(\"../pretrained_models/checkpoint_086.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
